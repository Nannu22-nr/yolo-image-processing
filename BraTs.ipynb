{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pdb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    # Apply sigmoid to each prediction (for multilabel or binary classification)\n",
    "    y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "\n",
    "    # Convert probabilities to binary labels (threshold > 0.5)\n",
    "    y_pred_tags = (y_pred_sigmoid > 0.5).float()\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "    # Convert to percentage\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained Vision Transformer model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",  # Pre-trained ViT model\n",
    "    num_labels=6  # Number of output classes\n",
    ")\n",
    "\n",
    "# Freeze the feature extraction layers\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier head with a custom classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.config.hidden_size, 256),  # ViT's hidden size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 6),  # Output layer for 6 classes\n",
    "    nn.Sigmoid()  # Change to Sigmoid for multilabel classification\n",
    ")\n",
    "\n",
    "# Print the updated classifier\n",
    "print(model.classifier)\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85,997,062 total parameters.\n",
      "198,406 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "# Define class weight if necessary. Otherwise use default\n",
    "#weights = [1, 1] # 1 weight for class 0 and 0.5 weight for class 1\n",
    "#class_weights=torch.FloatTensor(weights).cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#optimizer=optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code\n",
    "# Data loader and transformaiton\n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()]),\n",
    "        }\n",
    "\n",
    "# Place the train and test file directories in the root (Check the organization folder text file for details)\n",
    "BraTS_dataset = datasets.ImageFolder(root =r\"E:\\python\\deep learning\\fsjfr\\BraTs\\BraTs\",\n",
    "                                      transform = image_transforms[\"train\"]\n",
    "                                     )\n",
    "test_dataset = datasets.ImageFolder(root =r\"E:\\python\\deep learning\\fsjfr\\Test\",\n",
    "                                     transform = image_transforms[\"test\"]\n",
    ")\n",
    " #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 16821\n",
      "Train dataset size: 13456\n",
      "Validation dataset size: 3365\n",
      "Test dataset size: 1360\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Get the total size of the dataset\n",
    "# total_size = len(BraTS_dataset)\n",
    "\n",
    "# # Calculate the size of the subset (40% of the dataset)\n",
    "# subset_size = int(0.2 * total_size)\n",
    "\n",
    "# # Split the subset into train, validation, and test sets (70%, 20%, 10% of the 40%)\n",
    "# train_size = int(0.7 * subset_size)  # 70% of 40% for training\n",
    "# val_size = int(0.2 * subset_size)    # 20% of 40% for validation\n",
    "# test_size = subset_size - train_size - val_size  # Remaining for test\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "# subset_dataset, _ = random_split(BraTS_dataset, [subset_size, total_size - subset_size])\n",
    "\n",
    "# # Now, split the subset into train, validation, and test\n",
    "# train_dataset, val_dataset, test_dataset = random_split(subset_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "total_size = len(BraTS_dataset)\n",
    "\n",
    "# Split the BraTS dataset into train, validation sets (80%, 20%)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size  # Ensure all remaining data goes to validation\n",
    "\n",
    "# Split the dataset using `random_split`\n",
    "train_dataset, val_dataset = random_split(BraTS_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    # Access the logits attribute of the ImageClassifierOutput object\n",
    "    y_pred_sigmoid = F.logsigmoid(y_pred.logits)  # Use F.logsigmoid\n",
    "    _, y_pred_tags = torch.max(y_pred_sigmoid, dim=1)\n",
    "\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "\n",
    "    return acc\n",
    "\n",
    "#Change 1\n",
    "criterion = nn.CrossEntropyLoss() # use CrossEntropyLoss for multi-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m     train_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     train_epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_acc\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# VALIDATION\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # TRAINING\n",
    "    i = 0\n",
    "    min_val_loss = np.Inf\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_train_pred = model(X_train_batch)\n",
    "\n",
    "        #Change 2: Access the logits attribute from y_train_pred\n",
    "        train_loss = criterion(y_train_pred.logits, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "    # VALIDATION\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            y_val_pred = model(X_val_batch)\n",
    "\n",
    "            #Change 3: Access the logits attribute from y_val_pred\n",
    "            val_loss = criterion(y_val_pred.logits, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss / len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss / len(val_loader))\n",
    "    vLoss = val_epoch_loss / len(val_loader)\n",
    "\n",
    "    # If the validation loss is at a minimum\n",
    "    if vLoss < min_val_loss:\n",
    "      # Save the model\n",
    "      torch.save(model.state_dict(), \"net.pth\")  # OK\n",
    "      epochs_no_improve = 0\n",
    "      min_val_loss = vLoss\n",
    "\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
    "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"net.pth\"))\n",
    "# TEST\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = model(x_batch)\n",
    "        # Change here: Apply softmax to the logits\n",
    "        y_test_pred = torch.softmax(y_test_pred.logits, dim=1)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())\n",
    "\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "# Evaluation\n",
    "# Convert the lists of arrays into 1D arrays\n",
    "y_pred_list = np.concatenate(y_pred_list)\n",
    "y_true_list = np.concatenate(y_true_list)\n",
    "\n",
    "\n",
    "# Confusion Matrix, Accuracy, F1-Score\n",
    "cm = confusion_matrix(y_true_list, np.argmax(y_pred_list, axis=1))\n",
    "print(cm)\n",
    "# Calculate metrics for multi-class classification\n",
    "FP = cm.sum(axis=0) - np.diag(cm)\n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "#TN, FP, FN, TP = confusion_matrix(y_true_list,y_pred_list).ravel()\n",
    "sensitivity = np.round((TP / (TP + FN)),25)\n",
    "specificity = np.round((TN / (FP + TN)),25)\n",
    "precision = np.round((TP / (TP + FP)),25)\n",
    "recall = np.round((TP / (TP + FN)),25)\n",
    "f1 = np.round(((2*precision*recall)/(precision+recall)),25)\n",
    "if np.isnan(f1).any():\n",
    "   f1 = np.nan_to_num(f1, nan=0)\n",
    "accuracy = np.round(((TP + TN) / len(y_true_list)),25)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true_list, y_pred_list, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
